import pandas as pd
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline
import matplotlib.pyplot as plt
import seaborn as sns



# 1. åŠ è½½æ ‡å‡†åŒ–æ•°æ®
df = pd.read_csv('Ready to Model.csv')

# 2. ç‰¹å¾ä¸æ ‡ç­¾
features = ['TA(t)/TA(t-1)', 'NP(t)/NP(t-1)', 'TL/TA', 'CA/CL',
            'TL/TSE', 'NP/ASE', 'SR/ACA', 'MBC/AI',
            'FA/TA', 'SE/FA', 'NP_SR']
X = df[features]
y = df['ST or Not']

# 3. æ‹†åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†ï¼ˆä¿æŒåˆ†å±‚æŠ½æ ·ï¼‰
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# 4. å®šä¹‰SMOTE + SVMçš„Pipeline
pipeline = Pipeline([
    ('svm', SVC(probability=True, random_state=42))  # å¯ç”¨æ¦‚ç‡ä¼°è®¡
])

# 5. ç½‘æ ¼æœç´¢å‚æ•°
# param_grid = {
#     'svm__C': [0.1, 1, 10],  # æ­£åˆ™åŒ–å‚æ•°
#     'svm__kernel': ['linear', 'rbf'],  # æ ¸å‡½æ•°
#     'svm__gamma': ['scale', 'auto']  # RBFæ ¸çš„å‚æ•°
# }

param_grid = {
    'svm__C': [0.1, 1, 10],  # æ­£åˆ™åŒ–å‚æ•°
    'svm__kernel': ['linear'],  # æ ¸å‡½æ•°

}


# 6. ç½‘æ ¼æœç´¢ï¼ˆæŒ‰AUCè¯„åˆ†ä¼˜åŒ–ï¼‰
random_search = RandomizedSearchCV(
    estimator=pipeline,
    param_distributions=param_grid,
    n_iter=20,  # ä»…éšæœºå°è¯•20æ¬¡ï¼ˆè¿œå°äºGridSearchçš„ç©·ä¸¾ï¼‰
    cv=5,
    scoring='roc_auc',
    random_state=42,
    n_jobs=-1
)

#å¼€å§‹è®­ç»ƒ
random_search.fit(X_train, y_train)



# 7. è·å–æœ€ä½³æ¨¡å‹
best_model = random_search.best_estimator_
print(f"\nğŸ¯ æœ€ä½³å‚æ•°ç»„åˆ: {random_search.best_params_}")

# 8. é¢„æµ‹ä¸è¯„ä¼°
y_pred = best_model.predict(X_test)
y_prob = best_model.predict_proba(X_test)[:, 1]  # æ­£ç±»æ¦‚ç‡

# 9. è¾“å‡ºè¯„ä¼°æŒ‡æ ‡
print("\nâœ… SVMæ¨¡å‹è¯„ä¼°ç»“æœï¼ˆSVMï¼‰")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"F1 Score: {f1_score(y_test, y_pred):.4f}")
print(f"AUC Score: {roc_auc_score(y_test, y_prob):.4f}")
print("\nğŸ“Š Classification Report:\n", classification_report(y_test, y_pred))

# 10. å¯è§†åŒ–æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(5, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - SVM with SMOTE')
plt.tight_layout()
plt.show()

# 11. ç‰¹å¾é‡è¦æ€§ï¼ˆä»…å½“ä½¿ç”¨çº¿æ€§æ ¸æ—¶ï¼‰
if best_model.named_steps['svm'].kernel == 'linear':
    coef_df = pd.DataFrame({
        'Feature': features,
        'Coefficient': best_model.named_steps['svm'].coef_[0]
    }).sort_values(by='Coefficient', key=abs, ascending=False)

    print("\nğŸ“Œ ç‰¹å¾é‡è¦æ€§ï¼ˆçº¿æ€§æ ¸çš„æƒé‡ç³»æ•°ï¼ŒæŒ‰ç»å¯¹å€¼æ’åºï¼‰:")
    print(coef_df)

    # å¯è§†åŒ–ç‰¹å¾æƒé‡
    plt.figure(figsize=(8, 5))
    sns.barplot(x='Coefficient', y='Feature', data=coef_df, palette='coolwarm')
    plt.title('SVM Feature Importance (Linear Kernel)')
    plt.axvline(0, color='gray', linestyle='--')
    plt.tight_layout()
    plt.show()
else:
    print("\nâš ï¸ æ³¨æ„ï¼šå½“å‰ä½¿ç”¨éçº¿æ€§æ ¸ï¼ˆ{}ï¼‰ï¼Œæ— æ³•ç›´æ¥è§£é‡Šç‰¹å¾é‡è¦æ€§".format(
        best_model.named_steps['svm'].kernel))